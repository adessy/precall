---
title: "Precall manual"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_caption: yes
    highlight: tango
    smart: yes
    theme: cosmo
    toc: yes
  pdf_document:
    toc: yes
  word_document: default
bibliography: bibliography.bib
vignette: |
  %\VignetteIndexEntry{Vignette Title} 
  %\VignetteEngine{knitr::rmarkdown} 
  \usepackage[utf8]{inputenc}
---

Precision and recall are two complementary performance measures used in the context of asymmetric binary classification.
"Asymmetric" means here that one is interested in identifying a subset of instances from the whole lot.
This is, for instance, the setting of information retrieval which aims at finding in a large set of documents a relevant subset with
respect to a given query.

Most of the methods applied in that context produce scores (e.g. probabilities, similarity scores) rather than pure classification.
The final classification is then obtained by ranking the instances according to their score and choosing a proper threshold.
Of course, this notion of proper threshold is very dependent of the application.
Indeed, different threshold values imply different trade-offs between the ability to identify accurately relevant instances (precision) and the ability to retrieve a large number of those instances (recall).

For the sake of clarity, let's play with a simple toy example. After applying an appropriate method on a set of 15 instances, we obtain the following ranking : P P N P P N P N N N N N N P N, where P indicates a Positive (relevant) sample, while N indicates a Negative (irrelevant) one. 
The first samples are considered more likely to be of interest than last ones (according to the method). 
A first threshold could be placed after the 7^th^ instance.
This leads to the identification of 7 samples among which 5 are positives; in other words, $precision = 5/7$. 
Also, among the 6 positive samples, 5 have been identified and thus, $recall = 5/6$.
By moving around the threshold, different trade-offs can be obtained between those two performance measures.
For instance, if it is critical not to include Negative samples, one can choose a more stringent threshold. 
But this will typically decrease the recall. 
For additional information about the definitions of precision and recall, and the related terminology, we refer the reader to @powers2007.

The `precall` package provides the utilities to plot the precision against the recall.
This is a parametric curve where both quantities are functions of the discrimination threshold.
The package can also be used to compute the area under this curve (AUPR).
This latter quantity is often used to compare different methods without having to choose a specific threshold. 
In this vignette, we demonstrate the main functionalities of the package using an expanded version of the [simple example](#simple-example) introduced earlier.

Finally, on a more technical note, most of a pr-curve is actually interpolated. 
Indeed, only the points corresponding to an actual split of the data are defined.
In the toy example proposed earlier, there are 15 instances and thus 16 possible splits, depending on the discrimination threshold.
The curve produced by `precall` are interpolated following a principle of partial acceptance detailed in [here](#interpolation-by-partial-acceptance).


## Simple example

All that is needed in order to draw a pr-curve and compute the AUPR is a ranking of positive and negative instances.
The ordering can be can be explicitly stated by means of scores or can be implicit as for the example discussed in the introduction: P P N P P N P N N N N N N P N.
This example can be readily translated into `R` by:

```{r, fig.width=8,fig.cap="**Figure 1: A simple pr-curve.**"}
labels = c(T,T,F,T,T,F,T,F,F,F,F,F,F,T,F)
precall::aupr(labels)
precall::plot_precall(labels)
```

To compare different ranking methods, a matrix of scores must be feeded to the function.
Each column of the score matrix specifies a ranking of the labels, which are sorted in ascending order of scores.
As demonstrated by the new rankings, several instances can have the same score and thus the same rank. 
These instances are included simultaneously while the discriminatory threshold moves on.
```{r, fig.width=8,fig.cap="**Figure 2: Multiple pr-curves on the same plot.**"}
scores = cbind(original=seq_along(labels),
               new1=c(1, 1, 1, 1, 3, 4, 7, 8, 8, 2, 6, 9, 9, 7, 4),
               new2=c(7, 8, 9, 9, 2, 3, 2, 6, 8, 4, 8, 4, 1, 7, 5))
precall::plot_precall(labels,scores)
precall::aupr(labels, scores)
```

Finally, basic aesthetic features of the plots can be adjusted thanks a few optional parameters.
```{r, fig.width=8, fig.cap="**Figure 3: Fancy pr-curves.**"}
precall::plot_precall(labels, scores, print_friendly = TRUE, use_color = TRUE)
```

## Interpolation by partial acceptance

Given a ranking of $N$ samples, its top-$k$ elements can be used to compute recall and precision.
The sequence of (precision, recall) points that forms the backbone of the curve is obtained by varying the value of $k$.
This entails that the number of points is upper bounded by the number of samples (less if there are ties in the ranking).
and, as a consequence, the final curve is obtained by interpolation.

The choice of the interpolation method is not simply a matter of aesthetic appeal since it has an impact on the AUPR.
The package `precall` interpolates the curve by using a principle of partial acceptance which is much more natural  and less arbitrary than joining the points with line segments.

Let's assume two consecutive points ($prec_1$, $rec_1$) and ($prec_2$, $rec_2$). 
These four quantities are defined as ratios : 
$$prec_i = \frac{tp_i}{k_i} \mbox{ and } rec_i = \frac{tp_i}{p} \mbox{ for } i \in \{1,2\},$$
where :

- $tp_i$ is the number of true positive samples; 
- $k_i$ is the sum of true and false positive samples;
- $p$ is the total number of positive samples.

The parametric representation of the interpolation between the two points is
$$prec(x) = \frac{tp_1 + (tp_2 - tp_1) \cdot x}{k_1 + (k_2 - k_1) \cdot x} \mbox{ and } rec(x) = \frac{tp_1 + (tp_2 - tp_1) \cdot x}{p}$$
with $x \in [0,1]$. 
*Partial acceptance* simply refers to the fact that an increasing proportion $x$ of the additional samples is progressively included into the ratios. 
In addition, if there are true positive samples among the top-ranked ones, the recall of the first point is not null. In order to obtain the start of curve, the same parametric expression is used with a virtual point ($prec_1$, $rec_1$) such that $tp_1 = 0$ and $k_1 = 0$. The expression hence simplifies to 
$$prec(x) = \frac{tp_2}{k_2} \mbox{ and } rec(x) = \frac{tp_2 \cdot x}{p} \mbox{ with } x \in [0,1].$$
As can be seen, it basically reduces to a constant. 
This behaviour can be observed, for instance, in Figure 2 for the *original* and *new1* curves.
For the *original* ranking, there is only one top-ranked sample which is positive. 
This corresponds to the first part^[The remaining of the horizontal segment arises from the fact that the ranking keeps on providing positive samples,
but the interpolation is now computed according to the first formulation.] of the horizontal segment (up to recall = 1/6).
In contrast, the top-ranked samples of the *new1* ranking consist of 3 positive and 1 negative samples (that are samples with a score equal to 1).
In that case, a constant precision is thus observed up to a recall of 3/6.

Finally, let's mention that `precall` relies on a closed-form solution of the integral for this type of interpolation in order to compute the AUPR.

## References

